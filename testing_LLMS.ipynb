{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import huggingface_hub\n",
    "from huggingface_hub import notebook_login\n",
    "import ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd37602d89794e79b8521fc408be27f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader  \n",
    "from transformers import AutoTokenizer, AutoModel, Trainer, TrainingArguments\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/tweet_text_2.csv\", encoding = 'cp1252') \n",
    "tweet_text = data['TWEET_TEXT']\n",
    "tweet_lengths = [len(tweet) for tweet in tweet_text]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finetuning BERT to see performance for predicting gasoline shortage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding=max, truncation=True)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizing data and creating trainign an test sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"tokenized_tweets\"] = data['TWEET_TEXT'].apply(lambda x: tokenizer(x))\n",
    "data = data[data.Shortage.notna()]\n",
    "train_data = data.sample(frac =0.8)\n",
    "test_data = data[~data.index.isin(train_data.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SNO</th>\n",
       "      <th>TWEET_TEXT</th>\n",
       "      <th>Shortage</th>\n",
       "      <th>Demand</th>\n",
       "      <th>Probable Shortage</th>\n",
       "      <th>tokenized_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27751</td>\n",
       "      <td>Can someone explain to me why everyone is fill...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>27836</td>\n",
       "      <td>I need gas so bad and I'm at work . Hopefully ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>28462</td>\n",
       "      <td>People kept telling me that at 4am there was g...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>28511</td>\n",
       "      <td>Coral Springs area: there's gas in Wiles Rd an...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>29155</td>\n",
       "      <td>I'm wasting gas driving around trying to find it</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3485</th>\n",
       "      <td>821295</td>\n",
       "      <td>@politelad69 No idea. We drove through to Flor...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3489</th>\n",
       "      <td>822605</td>\n",
       "      <td>My gas tank is barely hanging on to life here ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3490</th>\n",
       "      <td>822645</td>\n",
       "      <td>No gas, water and ice for miles. \\rOver 400k i...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3494</th>\n",
       "      <td>823895</td>\n",
       "      <td>Soooooo ain't no gas in Valdosta?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3497</th>\n",
       "      <td>824363</td>\n",
       "      <td>Why don't you gas me up ?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>700 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         SNO                                         TWEET_TEXT  Shortage  \\\n",
       "0      27751  Can someone explain to me why everyone is fill...       0.0   \n",
       "5      27836  I need gas so bad and I'm at work . Hopefully ...       0.0   \n",
       "14     28462  People kept telling me that at 4am there was g...       1.0   \n",
       "15     28511  Coral Springs area: there's gas in Wiles Rd an...       1.0   \n",
       "23     29155   I'm wasting gas driving around trying to find it       1.0   \n",
       "...      ...                                                ...       ...   \n",
       "3485  821295  @politelad69 No idea. We drove through to Flor...       0.0   \n",
       "3489  822605  My gas tank is barely hanging on to life here ...       0.0   \n",
       "3490  822645  No gas, water and ice for miles. \\rOver 400k i...       1.0   \n",
       "3494  823895                  Soooooo ain't no gas in Valdosta?       1.0   \n",
       "3497  824363                          Why don't you gas me up ?       0.0   \n",
       "\n",
       "      Demand   Probable Shortage                              tokenized_tweets  \n",
       "0         1.0                 1.0  [input_ids, token_type_ids, attention_mask]  \n",
       "5         1.0                 1.0  [input_ids, token_type_ids, attention_mask]  \n",
       "14        1.0                 1.0  [input_ids, token_type_ids, attention_mask]  \n",
       "15        1.0                 1.0  [input_ids, token_type_ids, attention_mask]  \n",
       "23        1.0                 1.0  [input_ids, token_type_ids, attention_mask]  \n",
       "...       ...                 ...                                          ...  \n",
       "3485      0.0                 0.0  [input_ids, token_type_ids, attention_mask]  \n",
       "3489      1.0                 0.0  [input_ids, token_type_ids, attention_mask]  \n",
       "3490      1.0                 1.0  [input_ids, token_type_ids, attention_mask]  \n",
       "3494      1.0                 1.0  [input_ids, token_type_ids, attention_mask]  \n",
       "3497      0.0                 0.0  [input_ids, token_type_ids, attention_mask]  \n",
       "\n",
       "[700 rows x 6 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explpring the layers of bert base case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertEmbeddings(\n",
       "  (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "  (position_embeddings): Embedding(512, 768)\n",
       "  (token_type_embeddings): Embedding(2, 768)\n",
       "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "        \n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = model\n",
    "        self.linear = nn.Linear(768,num_labels)\n",
    "        self.softmax = nn.Softmax(1)\n",
    "    def forward(self, x):\n",
    "        bert_output = model(x)\n",
    "        output = self.softmax(self.linear(bert_output))\n",
    "        return output\n",
    "   \n",
    "num_labels = 2    \n",
    "finetune_model = Model(num_labels)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will finetune only the last layer in the first model\n",
    "MODEL 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for params in finetune_model.parameters():\n",
    "    params.requires_grad = False\n",
    "    \n",
    "for params in finetune_model.linear.parameters():\n",
    "    params.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting parameters, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "loss = nn.CrossEntropyLoss()\n",
    "learning_rate = 1e-5\n",
    "optimizer = torch.optim.Adam(finetune_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataSet(Dataset):\n",
    "    def __init__(self, tokens, labels):\n",
    "        self.tokens = tokens\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        token = self.tokens[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return {\"token\" : token,  \"label\" : label}\n",
    "    \n",
    "train_dataset = CustomDataSet(train_data[\"tokenized_tweets\"], train_data['Shortage'])\n",
    "test_dataset = CustomDataSet(test_data[\"tokenized_tweets\"], test_data['Shortage'])\n",
    "train_dataloader = DataLoader(dataset=train_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(dataset=test_data, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
