{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import huggingface_hub\n",
    "from huggingface_hub import notebook_login\n",
    "import ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd37602d89794e79b8521fc408be27f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader  \n",
    "from transformers import AutoTokenizer, AutoModel, Trainer, TrainingArguments\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/tweet_text_2.csv\", encoding = 'cp1252') \n",
    "tweet_text = data['TWEET_TEXT']\n",
    "tweet_lengths = [len(tweet) for tweet in tweet_text]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finetuning BERT to see performance for predicting gasoline shortage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizing data and creating trainign an test sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def get_tokenized_data(row):\n",
    "#    input_ids, token_type_ids, attention_masks = tokenizer(row[\"TWEET_TEXT\"], padding = \"max_length\", truncation = True)\n",
    "#    row['input_ids'] = input_ids[\"input_ids\"]\n",
    "#    row['token_type_ids'] = token_type_ids[\"input_ids\"]\n",
    "#    row['attention_masks'] = attention_masks[\"\"]\n",
    "#    return row\n",
    "\n",
    "\n",
    "#data = data.apply(lambda row: get_tokenized_data(row), axis =1)\n",
    "data = data[data.Shortage.notna()]\n",
    "train_data = data.sample(frac =0.8)\n",
    "test_data = data[~data.index.isin(train_data.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SNO</th>\n",
       "      <th>TWEET_TEXT</th>\n",
       "      <th>Shortage</th>\n",
       "      <th>Demand</th>\n",
       "      <th>Probable Shortage</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>token_type_ids</th>\n",
       "      <th>attention_masks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2205</th>\n",
       "      <td>305773</td>\n",
       "      <td>I never thought I would have to drive around 3...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>input_ids</td>\n",
       "      <td>token_type_ids</td>\n",
       "      <td>attention_mask</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>48552</td>\n",
       "      <td>Time to try and find gas</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>input_ids</td>\n",
       "      <td>token_type_ids</td>\n",
       "      <td>attention_mask</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138</th>\n",
       "      <td>147005</td>\n",
       "      <td>I miss driving my cars and yelling at idiot dr...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>input_ids</td>\n",
       "      <td>token_type_ids</td>\n",
       "      <td>attention_mask</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3018</th>\n",
       "      <td>696130</td>\n",
       "      <td>Viewer Mike Ross sent me this picture of the S...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>input_ids</td>\n",
       "      <td>token_type_ids</td>\n",
       "      <td>attention_mask</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2927</th>\n",
       "      <td>597353</td>\n",
       "      <td>I swea yall gone feel me this year . All gas n...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>input_ids</td>\n",
       "      <td>token_type_ids</td>\n",
       "      <td>attention_mask</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070</th>\n",
       "      <td>140697</td>\n",
       "      <td>Consider using local roads if you're heading n...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>input_ids</td>\n",
       "      <td>token_type_ids</td>\n",
       "      <td>attention_mask</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3123</th>\n",
       "      <td>727748</td>\n",
       "      <td>I need some gas like yesterday</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>input_ids</td>\n",
       "      <td>token_type_ids</td>\n",
       "      <td>attention_mask</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>109299</td>\n",
       "      <td>Y'all say it's hard to find gas? I literally d...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>input_ids</td>\n",
       "      <td>token_type_ids</td>\n",
       "      <td>attention_mask</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2660</th>\n",
       "      <td>426291</td>\n",
       "      <td>I stop at every gas stations here and none of ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>input_ids</td>\n",
       "      <td>token_type_ids</td>\n",
       "      <td>attention_mask</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>97273</td>\n",
       "      <td>Heading to #Duvalcounty beaches for gas? Keep ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>input_ids</td>\n",
       "      <td>token_type_ids</td>\n",
       "      <td>attention_mask</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2799 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         SNO                                         TWEET_TEXT  Shortage  \\\n",
       "2205  305773  I never thought I would have to drive around 3...       1.0   \n",
       "222    48552                           Time to try and find gas       1.0   \n",
       "1138  147005  I miss driving my cars and yelling at idiot dr...       0.0   \n",
       "3018  696130  Viewer Mike Ross sent me this picture of the S...       1.0   \n",
       "2927  597353  I swea yall gone feel me this year . All gas n...       0.0   \n",
       "...      ...                                                ...       ...   \n",
       "1070  140697  Consider using local roads if you're heading n...       0.0   \n",
       "3123  727748                     I need some gas like yesterday       0.0   \n",
       "800   109299  Y'all say it's hard to find gas? I literally d...       0.0   \n",
       "2660  426291  I stop at every gas stations here and none of ...       0.0   \n",
       "695    97273  Heading to #Duvalcounty beaches for gas? Keep ...       0.0   \n",
       "\n",
       "      Demand   Probable Shortage   input_ids  token_type_ids attention_masks  \n",
       "2205      1.0                 1.0  input_ids  token_type_ids  attention_mask  \n",
       "222       1.0                 1.0  input_ids  token_type_ids  attention_mask  \n",
       "1138      1.0                 0.0  input_ids  token_type_ids  attention_mask  \n",
       "3018      1.0                 1.0  input_ids  token_type_ids  attention_mask  \n",
       "2927      0.0                 0.0  input_ids  token_type_ids  attention_mask  \n",
       "...       ...                 ...        ...             ...             ...  \n",
       "1070      1.0                 1.0  input_ids  token_type_ids  attention_mask  \n",
       "3123      1.0                 0.0  input_ids  token_type_ids  attention_mask  \n",
       "800       1.0                 1.0  input_ids  token_type_ids  attention_mask  \n",
       "2660      0.0                 0.0  input_ids  token_type_ids  attention_mask  \n",
       "695       0.0                 0.0  input_ids  token_type_ids  attention_mask  \n",
       "\n",
       "[2799 rows x 8 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explpring the layers of bert base case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "        \n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = model\n",
    "        self.linear = nn.Linear(768,num_labels)\n",
    "        self.softmax = nn.Softmax(1)\n",
    "    def forward(self,token_ids, attention_masks ):\n",
    "        _,bert_output = model(token_ids, attention_mask=attention_masks,return_dict=False)\n",
    "        output = self.softmax(self.linear(bert_output))\n",
    "        return output\n",
    "   \n",
    "num_labels = 2    \n",
    "finetune_model = Model(num_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will finetune only the last layer in the first model\n",
    "MODEL 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "for params in finetune_model.parameters():\n",
    "    params.requires_grad = False\n",
    "    \n",
    "for params in finetune_model.linear.parameters():\n",
    "    params.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting parameters, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 1e-5\n",
    "optimizer = torch.optim.Adam(finetune_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataSet(Dataset):\n",
    "    def __init__(self, token_ids, attention_masks, labels):\n",
    "        self.tokens = token_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokens[idx], self.attention_masks[idx], self.labels[idx]\n",
    "    \n",
    "train_sample = tokenizer.batch_encode_plus(list(train_data[\"TWEET_TEXT\"]), padding = \"max_length\", truncation=True)\n",
    "test_sample = tokenizer.batch_encode_plus(list(test_data[\"TWEET_TEXT\"]), padding = \"max_length\", truncation=True)\n",
    "train_dataset = CustomDataSet(torch.tensor(train_sample[\"input_ids\"]), torch.tensor(train_sample[\"attention_mask\"]), torch.tensor(list(train_data['Shortage'])))\n",
    "test_dataset = CustomDataSet(torch.tensor(test_sample[\"input_ids\"]), torch.tensor(test_sample[\"attention_mask\"]), torch.tensor(list(test_data['Shortage'])))\n",
    "#test_dataset = CustomDataSet(list(test_data[\"tokenized_tweets\"]), list(test_data['Shortage']))\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_sample[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "loss_adam = []\n",
    "n_epochs = 10\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_loss_train, epoch_loss_test, epoch_accuracy_train, epoch_acccuracy_test = 0, 0, 0, 0\n",
    "    outputs_train, outputs_test, labels_epochs_train, labels_epochs_test  = [], [], [], []\n",
    "    n = 0 \n",
    "    for step, batch in enumerate(train_dataloader): \n",
    "        n += 1\n",
    "        batch_loss = 0   \n",
    "        input_ids, attention_masks, labels = batch\n",
    "        output = finetune_model(input_ids, attention_masks)\n",
    "        loss = criterion(output, labels.long())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"epoch/total epoch : {epoch}/{n_epochs}---- step:{step}----- loss:{loss.item()}\")\n",
    "        epoch_loss_train += loss.item() \n",
    "        output = output.detach().numpy()\n",
    "        labels = output.detach().numpy()\n",
    "        outputs_train.append(output)\n",
    "        labels_epochs_train = labels_epochs_train.append(labels)\n",
    "    outputs_train = np.concatenate(outputs_train, axis =1)\n",
    "    label_epochs_train = np.concatenate(label_epochs_train, axis =1)\n",
    "    epoch_accuracy_train = sum(outputs_train == label_epochs_train)/(outputs_train.shape[0])\n",
    "    for step, batch in enumerate(test_dataloader):\n",
    "        input_ids, attention_masks, labels = batch\n",
    "        output = finetune_model(input_ids, attention_masks)\n",
    "        loss = criterion(output, labels.long())\n",
    "        epoch_loss_test += loss.item()\n",
    "        output = output.detach().numpy()\n",
    "        labels = output.detach().numpy()\n",
    "    outputs_test = np.concatenate(outputs_test, axis =1)\n",
    "    label_epochs_test = np.concatenate(label_epochs_test, axis =1)\n",
    "    epoch_accuracy_test = sum(outputs_test == label_epochs_test)/(outputs_test.shape[0])\n",
    "    print(f\"epoch/total epoch : {epoch}/{n_epochs} --- train loss:{epoch_loss_train/n} --- test loss:{epoch_loss_test/n} --- train accuracy:{epoch_accuracy_train} --- test accuracy:{epoch_accuracy_test}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.75)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a  = np.array([0,1,0,2])\n",
    "b = np.array([0,1,1,2])\n",
    "\n",
    "sum(a == b)/a.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
